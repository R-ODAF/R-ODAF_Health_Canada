import pandas as pd
from pathlib import Path
import os

# load and validate config stuff
configfile: "config/config.yaml"

common_config = config["common"]
pipeline_config = config["pipeline"]

# set up directories
main_dir = common_config["projectdir"]
if main_dir is None:
    main_dir = os.getcwd()
main_dir = Path(main_dir)


genome_dir = Path(pipeline_config["genomedir"])
num_threads = pipeline_config["threads"]

data_dir = main_dir / "data"
raw_dir = data_dir / "raw" # should already exist
metadata_dir = data_dir / "metadata" # should already exist
metadata_file = metadata_dir / "metadata.txt" # should aready exist
#TODO set up a check to make sure this stuff exists

if common_config["platform"] =="TempO-Seq":
    temposeq_quantification_script = main_dir / "scripts/temposeq/temposeq_quantification.R"
sample_id_col = pipeline_config["sample_id"]

SAMPLES = pd.read_table(metadata_file)[sample_id_col].tolist()
print("samples: " + str(SAMPLES))

# set up output dirs

processed_dir = data_dir / "processed"
processed_dir.mkdir(parents=True, exist_ok=True)

trim_dir = processed_dir / "trimmed"
align_dir = processed_dir / "aligned"
quant_dir = processed_dir / "quant"
trim_dir.mkdir(parents=True, exist_ok=True)
align_dir.mkdir(parents=True, exist_ok=True)
quant_dir.mkdir(parents=True, exist_ok=True)

analysis_dir = main_dir / "analysis"
analysis_dir.mkdir(parents=True, exist_ok=True)

qc_dir = analysis_dir / "QC"
qc_dir.mkdir(parents=True, exist_ok=True)

log_dir = main_dir / "logs"
log_dir.mkdir(parents=True, exist_ok=True)

print("using genome: " + str(genome_dir))

shell("cp -rf workflow {log_dir}")
shell("cp -rf config {log_dir}")

##########################
### run whole pipeline ###
##########################

rule all:
    input:
        "reports_complete"
    benchmark: log_dir / "benchmark.all.txt"
    shell:
      '''
      rm reports_complete
      '''

##################################
### Trimming raw reads : Fastp ###
##################################

if common_config["platform"] =="TempO-Seq":
    length_required_fastp = 50
    trim_tail1_fastp = 1
else:
    length_required_fastp = 36
    trim_tail1_fastp = 0

rule fastp_se:
    input:
        R1 = ancient(str(raw_dir / "{sample}.fastq.gz"))
    output:
        R1 = pipe(trim_dir / "{sample}.fastq.gz"),
        json = trim_dir / "{sample}_fastp.json",
        html = trim_dir / "{sample}_fastp.html"
    conda:
        "envs/preprocessing.yml"
    params:
        front_size = 1,
        front_quality = 3,
        tail_size = 1,
        tail_quality = 3,
        right_size = 4,
        right_quality = 15,
        length_required = length_required_fastp,
        trim_tail1 = trim_tail1_fastp
    benchmark: log_dir / "benchmark.{sample}.fastp_se.txt"
    threads: workflow.cores
    shell:
        '''
        fastp \
        --in1 {input.R1} \
        --out1 {output.R1} \
        --json {output.json} \
        --html {output.html} \
        --cut_front \
        --disable_adapter_trimming \
        --cut_front_window_size {params.front_size} \
        --cut_front_mean_quality {params.front_quality} \
        --cut_tail \
        --cut_tail_window_size {params.tail_size} \
        --cut_tail_mean_quality {params.tail_quality} \
        --cut_right \
        --cut_right_window_size {params.right_size} \
        --cut_right_mean_quality {params.right_quality} \
        --length_required {params.length_required} \
        --trim_tail1 {params.trim_tail1} \
        --thread {threads} \
        '''


rule fastp_pe:
    input:
        R1 = ancient(str(raw_dir / "{sample}.R1.fastq.gz")),
        R2 = ancient(str(raw_dir / "{sample}.R2.fastq.gz"))
    output:
        R1 = pipe(trim_dir / "{sample}.R1.fastq.gz"),
        R2 = pipe(trim_dir / "{sample}.R2.fastq.gz"),
        json = trim_dir / "{sample}_fastp.json",
        html = trim_dir / "{sample}_fastp.html"
    conda:
        "envs/preprocessing.yml"
    params:
        front_size = 1,
        front_quality = 3,
        tail_size = 1,
        tail_quality = 3,
        right_size = 4,
        right_quality = 15,
        length_required = length_required_fastp
    benchmark: log_dir / "benchmark.{sample}.fastp_pe.txt"
    threads: workflow.cores
    shell:
        '''
        fastp \
        --in1 {input.R1} \
        --in2 {input.R2} \
        --out1 {output.R1} \
        --out2 {output.R2} \
        --json {output.json} \
        --html {output.html} \
        --cut_front \
        --cut_front_window_size {params.front_size} \
        --cut_front_mean_quality {params.front_quality} \
        --cut_tail \
        --cut_tail_window_size {params.tail_size} \
        --cut_tail_mean_quality {params.tail_quality} \
        --cut_right \
        --cut_right_window_size {params.right_size} \
        --cut_right_mean_quality {params.right_quality} \
        --length_required {params.length_required} \
        --thread {threads} \
        '''

################################
### Alignment of reads: STAR ###
################################

if common_config["platform"] =="TempO-Seq":
    STAR_index = genome_dir / "STAR_index"
    STAR_insertion_deletion_penalty = -1000000 # STAR scoring penalty for deletion/insertion, set by biospyder
    STAR_genomeSAindexNbases = 4 # Non-default as specified by BioSpyder
    STAR_multimap_nmax = 1
    STAR_mismatch_nmax = 2
else:
    STAR_index = genome_dir / "STAR_index"
    STAR_insertion_deletion_penalty = -2 # STAR defaults
    STAR_genomeSAindexNbases = 14 # STAR defaults
    STAR_multimap_nmax = 20
    STAR_mismatch_nmax = 999

# Build STAR index if not already present
rule STAR_make_index:
    input:
        genome = ancient(genome_dir / pipeline_config["genome_filename"])
    params:
        index_dir = STAR_index,
        annotations = genome_dir / pipeline_config["annotation_filename"],
        overhang = 100,
        suffix_array_sparsity = 2, # bigger for smaller (RAM), slower indexing
        genomeChrBinNbits = 18, # might need to mess with this for different genomes - default is 18
        genomeSAindexNbases = STAR_genomeSAindexNbases,
        sjdbGTFfeatureExon = "exon" # STAR default
    conda:
        "envs/preprocessing.yml"
    output:
        directory(genome_dir / "STAR_index")
    benchmark: log_dir / "benchmark.STAR_make_index.txt"
    threads: workflow.cores
    shell:
        '''
		STAR \
    --runMode genomeGenerate \
		--genomeDir {params.index_dir} \
		--genomeFastaFiles {input.genome} \
		--sjdbGTFfile {params.annotations} \
		--sjdbOverhang {params.overhang} \
		--runThreadN {threads} \
		--genomeSAsparseD {params.suffix_array_sparsity} \
		--genomeChrBinNbits {params.genomeChrBinNbits} \
		--genomeSAindexNbases {params.genomeSAindexNbases} \
		--sjdbGTFfeatureExon {params.sjdbGTFfeatureExon}
        '''


# this rule loads the genome into shared memory, and uses
# the 'service' keyword introduced in v7.0.
# Note that the genome.fasta.index is a dummy file
# https://snakemake.readthedocs.io/en/latest/snakefiles/rules.html#service-rules-jobs
# https://github.com/snakemake/snakemake/pull/1413
rule STAR_load:
    input:
        genome_dir / "STAR_index"
    output:
        touch("genome.loaded")
    conda:
        "envs/preprocessing.yml"
    params:
        index = STAR_index
    benchmark: log_dir / "benchmark.STAR_load.txt"
    shell:
        '''
        STAR --genomeLoad LoadAndExit --genomeDir {params.index}
        rm -f genome.removed
        '''

rule STAR_all:
    input:
        expand(str(align_dir / "{sample}.Aligned.toTranscriptome.out.bam"), sample=SAMPLES),
        "genome.removed"

if pipeline_config["mode"] == "se":
    rule STAR:
        input:
            loaded_index = "genome.loaded",
            R1 = trim_dir / "{sample}.fastq.gz"
        output:
            sortedByCoord = align_dir / "{sample}.Aligned.sortedByCoord.out.bam",
            toTranscriptome = align_dir / "{sample}.Aligned.toTranscriptome.out.bam"
        conda:
            "envs/preprocessing.yml"
        params:
            index = STAR_index,
            penalty = STAR_insertion_deletion_penalty,
            multimap_nmax = STAR_multimap_nmax,
            mismatch_nmax = STAR_mismatch_nmax,
            annotations = genome_dir / pipeline_config["annotation_filename"],
            folder = "{sample}",
            bam_prefix = lambda wildcards : align_dir / "{}.".format(wildcards.sample)
        benchmark: log_dir / "benchmark.{sample}.STAR_pe.txt"
        threads: workflow.cores
        shell:
            '''
            [ -e /tmp/{params.folder} ] && rm -r /tmp/{params.folder}
            STAR \
                --alignEndsType EndToEnd \
                --genomeLoad LoadAndKeep \
                --runThreadN {threads} \
                --genomeDir {params.index} \
                --readFilesIn {input.R1} \
                --quantMode TranscriptomeSAM \
                --limitBAMsortRAM=10737418240 \
                --outTmpDir /tmp/{params.folder} \
                --scoreDelOpen {params.penalty} \
                --scoreInsOpen {params.penalty} \
                --outFilterMultimapNmax {params.multimap_nmax} \
                --outFilterMismatchNmax {params.mismatch_nmax} \
                --readFilesCommand zcat \
                --outFileNamePrefix {params.bam_prefix} \
                --outSAMtype BAM SortedByCoordinate
            '''

if pipeline_config["mode"] == "pe":
    rule STAR:
        input:
            loaded_index = "genome.loaded",
            R1 = trim_dir / "{sample}.R1.fastq.gz",
            R2 = trim_dir / "{sample}.R2.fastq.gz"
        output:
            sortedByCoord = align_dir / "{sample}.Aligned.sortedByCoord.out.bam",
            toTranscriptome = align_dir / "{sample}.Aligned.toTranscriptome.out.bam"
        conda:
            "envs/preprocessing.yml"
        params:
            index = STAR_index,
            annotations = genome_dir / pipeline_config["annotation_filename"],
            folder = "{sample}",
            bam_prefix = lambda wildcards : align_dir / "{}.".format(wildcards.sample)
        resources:
            load=100
        benchmark: log_dir / "benchmark.{sample}.STAR_se.txt"
        threads: workflow.cores
        shell:
            '''
            [ -e /tmp/{params.folder} ] && rm -r /tmp/{params.folder}
            STAR \
            --genomeLoad LoadAndKeep \
            --runThreadN {threads} \
            --genomeDir {params.index} \
            --readFilesIn {input.R1} {input.R2} \
            --quantMode TranscriptomeSAM \
            --readFilesCommand zcat \
            --outFileNamePrefix {params.bam_prefix} \
            --outTmpDir /tmp/{params.folder} \
            --outSAMtype BAM SortedByCoordinate
            '''

rule STAR_unload:
    input:
        idx = "genome.loaded",
        bams = expand(str(align_dir / "{sample}.Aligned.toTranscriptome.out.bam"), sample=SAMPLES)
    output:
        touch("genome.removed")
    conda:
        "envs/preprocessing.yml"
    params:
        genome_dir = STAR_index
    shell:
        '''
        STAR --genomeLoad Remove --genomeDir {params.genome_dir}
        rm genome.loaded
        rm Log.progress.out Log.final.out Log.out SJ.out.tab Aligned.out.sam
        '''



########################
# Quantification QuasR #
#   (TempOSeq only)    #
########################

if common_config["platform"] =="TempO-Seq":
    rule index_all:
        input:
            expand(str(align_dir / "{sample}.Aligned.sortedByCoord.out.bam.bai"), sample=SAMPLES)

    rule samtools_index:
        input:
            aligned = align_dir / "{sample}.Aligned.sortedByCoord.out.bam"
        output:
            index = align_dir / "{sample}.Aligned.sortedByCoord.out.bam.bai"
        conda:
            "envs/temposeqr.yml"
        benchmark: log_dir / "benchmark.{sample}.samtools_index.txt"
        shell:
            '''
            samtools index {input.aligned}
            '''

    rule quantification_input:
        input:
            list(expand(str(align_dir / "{sample}.Aligned.sortedByCoord.out.bam"), sample=SAMPLES))
        output:
            samplefile = processed_dir / "samplefile.txt"
        benchmark: log_dir / "benchmark.quantification_input.txt"
        run:
            print("samples: " + str(SAMPLES))
            print("filenames: " + str(input))
            df = pd.DataFrame({"FileName": input, "SampleName": SAMPLES})
            df.to_csv(output.samplefile, index=False, sep='\t')

    rule quantification:
        input:
            aligned = expand(str(align_dir / "{sample}.Aligned.sortedByCoord.out.bam"), sample=SAMPLES),
            index = expand(str(align_dir / "{sample}.Aligned.sortedByCoord.out.bam.bai"), sample=SAMPLES),
            samplefile = processed_dir / "samplefile.txt"
        params:
            annotfile = str(genome_dir / pipeline_config["annotation_filename"]),
            threads = workflow.cores,
            genome = str(genome_dir / pipeline_config["genome_filename"])
        threads: workflow.cores
        output:
            count_table = processed_dir / "count_table.tsv"
        conda:
            "envs/temposeqr.yml"
        benchmark: log_dir / "benchmark.quantification.txt"
        shell:
            '''
            Rscript \
            {temposeq_quantification_script} \
            {input.samplefile} \
            {params.genome} \
            {params.annotfile} \
            {output.count_table} \
            {params.threads}
            '''



#######################
# QUANTIFICATION RSEM #
#   (RNA-seq only)    #
#######################

if common_config["platform"] =="RNA-Seq":
    rule RSEM_make_index:
        input:
            genome = genome_dir / pipeline_config["genome_filename"]
        params:
            annotations = genome_dir / pipeline_config["annotation_filename"],
            genome_name = pipeline_config["genome_name"],
            genome_dir = pipeline_config["genomedir"]
        output:
            directory(genome_dir / "RSEM_index")
        conda:
            "envs/preprocessing.yml"
        benchmark: log_dir / "benchmark.RSEM_make_index.txt"
        shell:
            '''
            mkdir {params.genome_dir}/RSEM_index
            rsem-prepare-reference --gtf {params.annotations} {input.genome} {output}/{params.genome_name}
            '''

    if pipeline_config["mode"] == "pe":
        rule RSEM:
            input:
                bam = align_dir / "{sample}.Aligned.toTranscriptome.out.bam",
                index = genome_dir / "RSEM_index"
            output:
                isoforms = quant_dir / "{sample}.isoforms.results",
                genes = quant_dir / "{sample}.genes.results"
            conda:
                "envs/preprocessing.yml"
            params:
                threads = workflow.cores,
                output_prefix =  lambda wildcards : quant_dir / "{}".format(wildcards.sample),
                index_name = pipeline_config["genome_name"]
            benchmark: log_dir / "benchmark.{sample}.RSEM_pe.txt"
            threads: workflow.cores
            shell:
                '''
                rsem-calculate-expression \
                -p {params.threads} \
                --paired-end \
                --bam {input.bam} \
                --no-bam-output \
                {input.index}/{params.index_name} \
                {params.output_prefix}
                '''

    if pipeline_config["mode"] == "se":
        rule RSEM:
            input:
                bam = align_dir / "{sample}.Aligned.toTranscriptome.out.bam",
                index = genome_dir / "RSEM_index"
            output:
                isoforms = quant_dir / "{sample}.isoforms.results",
                genes = quant_dir / "{sample}.genes.results"
            conda:
                "envs/preprocessing.yml"
            params:
                threads = workflow.cores,
                output_prefix =  lambda wildcards : quant_dir / "{}".format(wildcards.sample),
                index_name = pipeline_config["genome_name"]
            benchmark: log_dir / "benchmark.{sample}.RSEM_se.txt"
            threads: workflow.cores
            shell:
                '''
                rsem-calculate-expression \
                -p {params.threads} \
                --bam {input.bam} \
                --no-bam-output \
                {input.index}/{params.index_name} \
                {params.output_prefix}
                '''

    rule counts_matrix:
        input:
            genes = expand(str(quant_dir / "{sample}.genes.results"), sample=SAMPLES),
            isoforms = expand(str(quant_dir / "{sample}.isoforms.results"), sample=SAMPLES)
        output:
            genes = processed_dir / "count_table.tsv",
            isoforms = processed_dir / "isoforms_table.tsv"
        conda:
            "envs/preprocessing.yml"
        benchmark: log_dir / "benchmark.counts_matrix.txt"
        shell:
            '''
            rsem-generate-data-matrix {input.genes} > {output.genes}
            sed -i 's/\.genes.results//g' {output.genes}
            sed -i 's|{quant_dir}/||g' {output.genes}
            sed -i 's/"//g' {output.genes}
            rsem-generate-data-matrix {input.isoforms} > {output.isoforms}
            sed -i 's/\.isoforms.results//g' {output.isoforms}
            sed -i 's|{quant_dir}/||g' {output.isoforms}
            sed -i 's/"//g' {output.isoforms}
            '''




###############
### MultiQC ###
###############


rule multiqc:
    message: "running multiqc for temposeq data"
    input:
        expand(str(align_dir / "{sample}.Aligned.toTranscriptome.out.bam"), sample=SAMPLES),
        expand(str(trim_dir / "{sample}_fastp.json"), sample=SAMPLES)
    output:
        qc_dir / "MultiQC_Report.html"
    conda:
        "envs/preprocessing.yml"
    benchmark: log_dir / "benchmark.multiqc.txt"
    shell:
        '''
        multiqc \
        --cl_config "extra_fn_clean_exts: {{ '_fastp.json' }}" \
        --cl_config "sample_names_replace_exact: True" \
        --filename MultiQC_Report.html \
        --interactive \
        --sample-names {metadata_file} \
        -m fastp -m star -m rsem \
        -fz {raw_dir} {trim_dir} {align_dir} {quant_dir} {processed_dir}
        mv MultiQC_Report.html MultiQC_Report_data.zip {qc_dir}
        '''

#####################
### Study-wide QC ###
#####################

rule studywideqc:
    message: "generating study-wide QC report in R..."
    input:
        qc_dir / "MultiQC_Report.html",
        processed_dir / "count_table.tsv",
        "genome.removed"
    output:
        qc_dir / "details/samples_removed.txt"
    conda:
        "envs/reports.yml"
    benchmark: log_dir / "benchmark.studywide_qc.txt"
    shell:
        '''
        Rscript scripts/render_studywide_QC_report.R
        '''

##############
### DESeq2 ###
##############

rule deseq2:
    message: "running DESeq2..."
    input:
        qc_dir / "details/samples_removed.txt"
    output:
        touch("DESeq2_complete")
    conda:
        "envs/reports.yml"
    benchmark: log_dir / "benchmark.deseq2.txt"
    shell:
        '''
        rm genome.removed
        Rscript scripts/run_DESeq2.R 
        '''

######################
### DESeq2 Reports ###
######################

rule deseq_reports:
    message: "generating DESeq2 reports in R..."
    input:
        "DESeq2_complete"
    output:
        touch("reports_complete")
    conda:
        "envs/reports.yml"
    benchmark: log_dir / "benchmark.deseq_report.txt"
    shell:
        '''
        rm DESeq2_complete
        Rscript scripts/render_DESeq2_report.parallel.R
        '''



###############
### Cleanup ###
###############
onerror:
    print("An error occurred. Attempting to unload genome")
    shell("rm Log.progress.out Log.final.out Log.out SJ.out.tab Aligned.out.sam")

